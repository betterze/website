<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zongze Wu</title>

    <meta name="author" content="Zongze Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zongze Wu
                </p>
                <p>I'm a research scientist/engineer at <a href="https://research.adobe.com/">Adobe Research</a> in San Francisco.
                </p>
                <p>
                  At Adobe, I work with <a href="https://www.adobe.com/products/firefly.html">FireFly team</a> for <a href="https://www.youtube.com/watch?v=dj27JV9WOyw">Structure Reference</a>.
                </p>
                <p>
                  I got my PhD degree at Hebrew University of Jerusalem in 2022, under supervision of Prof. Dani Lischinski and Eli Shechtman from Adobe Research. 
                  I got my bachelor degree at Tongji University in 2016.
                </p>
                <p style="text-align:center">
                  <a href="zongze.wu@mail.huji.ac.il">Email</a> &nbsp;/&nbsp;
                  <a href="https://docs.google.com/document/d/1nA9BjbzGH5CaL2SCBd2_lkMdSCuRxZE82HLNRfPy7kk/edit?usp=sharing">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=V8FwQGkAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/zongze_wu">Twitter</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/self.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/self.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My main interests are in generative modelings (GenAI), including diffusion model, GAN, and autoregressive model. 
                  I work on multi-modality generation/editing tasks in including image, video and text.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='r2r_image'>
            <img src='images/cat2dog.gif' width=100%>
          </div>
        </div>
      
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://betterze.github.io/TurboEdit/">
          <span class="papertitle">Turboedit: Instant text-based image editing.</span>
        </a>
        <br>
        <strong>Zongze Wu</strong>, 
        <a href="https://home.ttic.edu/~nickkolkin/home.html">Nicholas Kolkin</a>,
        <a href="https://www.linkedin.com/in/jonathan-brandt-23b334/">Jonathan Brandt</a>,
        <a href="https://richzhang.github.io/">Richard Zhang</a>,
        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a> 
        <br>
        <em>ECCV 2024</em>, 
        <br>
        <a href="https://betterze.github.io/TurboEdit/">project page</a>
        /
        <a href="https://arxiv.org/abs/2408.08332">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=1LG2xCTRc3s&t=1s">Video</a>
        <p></p>
        <p>
          Users can upload an image, and edit the image with natural language. Each edit only takes half a second.
				</p>
      </td>
    </tr>


    <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          
          <img src='images/lazy_diffusion.jpg' width=100%>
        </div>
        
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://lazydiffusion.github.io/">
          <span class="papertitle">Lazy diffusion transformer for interactive image editing.</span>
        </a>
        <br>
        <a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>,
        <strong>Zongze Wu</strong>,
        <a href="https://richzhang.github.io/">Richard Zhang</a>,
        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>,
        <a href="https://taesung.me/">Taesung Park</a>,
        <a href="https://mgharbi.com/">Michaël Gharbi</a>
        <br>
        <em>ECCV 2024</em>
        <br>
        <a href="https://lazydiffusion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2404.12382">arXiv</a>
        <p></p>
        <p>
        Instead of generating the entire image, we only generate the mask region to facilitate fast inpaint task.
        </p>
      </td>
    </tr>



    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/stylegan3.jpg' width="160">
        </div>
       
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/yuval-alaluf/stylegan3-editing">
			<span class="papertitle">Third time’s the charm? image and video editing with stylegan3.</span>
        </a>
        <br>
        
				<a href="https://yuval-alaluf.github.io/">Yuval Alaluf</a>,
				<a href="https://orpatashnik.github.io/">Or Patashnik</a>,
        <strong>Zongze Wu</strong>,
				Asif Zamir,
				<a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
				<a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
				<a href="https://danielcohenor.com/">Daniel Cohen-Or</a>,
        <br>
        <em>AIM ECCVW 2022</em>,
        <br>
        <a href="https://github.com/yuval-alaluf/stylegan3-editing">project page</a>
        /
        <a href="https://arxiv.org/abs/2201.13433">arXiv</a>
        <p></p>
        <p>
				We show StyleGAN3 can be trained with unaligned image, and its w/w+ spaces are entangled than  StyleGAN2.
        </p>
      </td>
    </tr>


    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/stylealign.gif' width=100%>
					</div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/betterze/StyleAlign">
			<span class="papertitle">Stylealign: Analysis and applications of aligned stylegan models.
</span>
        </a>
        <br>
				<strong>Zongze Wu</strong>,
				<a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>,
				<a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
        <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>
				<br>
        <em>ICLR 2022</em> <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://github.com/betterze/StyleAlign">project page</a>
        /
        <a href="https://arxiv.org/abs/2110.11323">arXiv</a>
        <p></p>
        <p>
          The child model's latent spaces are semantically aligned with its parent's, 
          inheriting incredibly rich semantics.</p>
      </td>

      <tr onmouseout="ever_stop()" onmouseover="ever_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='ever_image'>
              <img src='images/StyleCLIP_gif.gif' width=100%>
            </div>
          </div>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://github.com/orpatashnik/StyleCLIP">
        <span class="papertitle">Styleclip: Text-driven manipulation of stylegan imagery.
  </span>
          </a>
          <br>
          <a href="https://orpatashnik.github.io/">Or Patashnik*</a>,
          <strong>Zongze Wu*</strong>,
          <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
          <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>,
          <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>
          <br>
          <em>ICCV 2021</em> <font color="red"><strong>(Oral Presentation)</strong></font>
          <br>
          <a href="https://github.com/orpatashnik/StyleCLIP">project page</a>
          /
          <a href="https://arxiv.org/abs/2103.17249">arXiv</a>
          /
          <a href="https://www.youtube.com/watch?v=PhR1gpXDu0w">ICCV Video</a>
          /
          <a href="https://www.youtube.com/watch?v=5icI0NgALnQ">Demo Video</a>
          <p></p>
          <p>
            Text-based image editing through mapping CLIP space to StyleGAN latent space.</p>
        </td>

            <tr onmouseout="ever_stop()" onmouseover="ever_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ever_image'>
                    <img src='images/stylespace.jpg' width=100%>
                  </div>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://github.com/betterze/StyleSpace">
              <span class="papertitle">Stylespace analysis: Disentangled controls for stylegan image generation.
        </span>
                </a>
                <br>
                <strong>Zongze Wu*</strong>,
                <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
                <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                <br>
                <em>CVPR 2021</em> <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://github.com/betterze/StyleSpace">project page</a>
                /
                <a href="https://arxiv.org/abs/2011.12799">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=U7qRotRGr1w&feature=youtu.be"> Video</a>
                <p></p>
                <p>
                  The space of channel-wise style parameters is significantly more disentangled than the other intermediate latent spaces in StyleGAN.</p>
              
                </td>
    



                <tr onmouseout="ever_stop()" onmouseover="ever_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='ever_image'>
                        <img src='images/fg_retrieval.jpg' width=100%>
                      </div>
                    </div>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Wu_Fine-Grained_Foreground_Retrieval_via_Teacher-Student_Learning_WACV_2021_paper.pdf">
                  <span class="papertitle">Fine-grained foreground retrieval via teacher-student learning.
            </span>
                    </a>
                    <br>
                    <strong>Zongze Wu*</strong>,
                    <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
                    <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                    <br>
                    <em>WACV 2021</em> 
                    <br>
                    <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Wu_Fine-Grained_Foreground_Retrieval_via_Teacher-Student_Learning_WACV_2021_paper.pdf">arXiv</a>
                    /
                    <a href="https://www.youtube.com/watch?v=C93qoxfxdT8"> Video</a>
                    <p></p>
                    <p>
                      Retrieve foreground images that are semantically compatible with the background.
                    </td>



            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/website">webpage template from Jon Barron</a> 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
